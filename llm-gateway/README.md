# LLM Gateway

Gateway tÃ­ch há»£p vÃ  chuáº©n hÃ³a giao diá»‡n tÆ°Æ¡ng tÃ¡c cho cÃ¡c MÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n Ä‘a ná»n táº£ng.

## Features

- ğŸ”„ **Retry Logic**: Automatic retry with exponential backoff
- âœ… **Validation**: Zod schema validation for all requests
- ğŸ“Š **Logging**: Request tracking with Winston
- ğŸ”€ **Multi-Provider**: 12 LLM providers vá»›i free tier

## Supported Providers (Free Tier)

| Provider              | Base URL             | Free Tier          |
| --------------------- | -------------------- | ------------------ |
| OpenRouter            | openrouter.ai        | 30+ free models    |
| Google AI Studio      | ai.google.dev        | 1,500 req/day      |
| NVIDIA NIM            | build.nvidia.com     | Developer access   |
| Mistral               | console.mistral.ai   | 1B tokens/month    |
| Codestral             | codestral.mistral.ai | Code generation    |
| HuggingFace           | huggingface.co       | ~100 req/hour      |
| Groq                  | console.groq.com     | 14,400 req/day     |
| Cerebras              | cloud.cerebras.ai    | Free API key       |
| Cohere                | cohere.com           | Trial API          |
| GitHub Models         | github.com           | Free with GitHub   |
| Cloudflare Workers AI | cloudflare.com       | 10,000 neurons/day |
| Vertex AI             | cloud.google.com     | $300 credits       |

## Quick Start

```bash
# Install dependencies
npm install

# Copy environment file
cp .env.example .env

# Add your API keys to .env

# Run development server
npm run dev
```

## API Endpoints

### Chat Completion

```bash
POST /v1/chat/completions
```

Request body:

```json
{
  "provider": "groq",
  "model": "llama-3.3-70b-versatile",
  "messages": [{ "role": "user", "content": "Hello!" }],
  "temperature": 0.7,
  "max_tokens": 1000,
  "stream": false
}
```

### List Models

```bash
GET /v1/models                  # All models from all providers
GET /v1/models/providers        # List available providers
GET /v1/models/:provider        # Models from specific provider
```

### Health Check

```bash
GET /health
```

Response:

```json
{
  "status": "ok",
  "timestamp": "2025-01-05T...",
  "version": "1.0.0",
  "providers": { "total": 12, "active": 3 }
}
```

## Project Structure

```
llm-gateway/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.ts              # Entry point
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ index.ts          # Environment config
â”‚   â”‚   â””â”€â”€ validation.ts     # Zod schemas
â”‚   â”œâ”€â”€ types/                # TypeScript types
â”‚   â”œâ”€â”€ routes/               # API routes
â”‚   â”œâ”€â”€ providers/            # LLM provider adapters
â”‚   â”‚   â”œâ”€â”€ base.ts           # Base provider class
â”‚   â”‚   â”œâ”€â”€ factory.ts        # Provider factory
â”‚   â”‚   â””â”€â”€ ...               # Provider implementations
â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â”œâ”€â”€ validation.ts     # Request validation
â”‚   â”‚   â”œâ”€â”€ errorHandler.ts   # Error handling
â”‚   â”‚   â””â”€â”€ requestLogger.ts  # Request logging
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.ts         # Winston logger
â”‚       â””â”€â”€ retry.ts          # Retry logic
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â””â”€â”€ .env.example
```

## License

MIT
