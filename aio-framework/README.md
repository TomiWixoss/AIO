# AIO Framework

**All-In-One LLM Framework** - Multi-provider LLM integration v·ªõi auto-fallback v√† priority management cho JavaScript/TypeScript.

## ‚ú® T√≠nh nƒÉng

- üîÑ **Multi-Provider**: H·ªó tr·ª£ 4 providers (OpenRouter, Groq, Cerebras, Google AI)
- üéØ **Priority Management**: Qu·∫£n l√Ω ƒë·ªô ∆∞u ti√™n cho providers, models v√† API keys
- üîÅ **Auto Fallback**: T·ª± ƒë·ªông chuy·ªÉn sang provider/model kh√°c khi fail
- üîë **Key Rotation**: T·ª± ƒë·ªông th·ª≠ c√°c API keys kh√°c khi key hi·ªán t·∫°i fail
- üìä **Flexible Modes**: 
  - **Auto Mode**: T·ª± ƒë·ªông ch·ªçn provider/model theo priority
  - **Direct Mode**: Ch·ªâ ƒë·ªãnh c·ª• th·ªÉ provider v√† model
- üåä **Streaming**: H·ªó tr·ª£ streaming responses
- üí™ **TypeScript**: Full TypeScript support v·ªõi type definitions

## üì¶ C√†i ƒë·∫∑t

```bash
npm install @aio/llm-framework
```

## üöÄ Quick Start

### 1. Basic Usage (Direct Mode)

```typescript
import { AIO } from "@aio/llm-framework";

const aio = new AIO({
  providers: [
    {
      provider: "groq",
      apiKeys: [{ key: "gsk_xxx" }],
      models: [{ modelId: "llama-3.3-70b-versatile" }],
    },
  ],
  autoMode: false,
});

const response = await aio.chatCompletion({
  provider: "groq",
  modelId: "llama-3.3-70b-versatile",
  messages: [
    { role: "user", content: "Hello!" },
  ],
});

console.log(response.content);
```

### 2. Auto Mode v·ªõi Fallback

```typescript
const aio = new AIO({
  providers: [
    {
      provider: "groq",
      apiKeys: [{ key: "gsk_xxx" }],
      models: [{ modelId: "llama-3.3-70b-versatile" }],
      priority: 10, // ∆Øu ti√™n cao nh·∫•t
    },
    {
      provider: "cerebras",
      apiKeys: [{ key: "csk_xxx" }],
      models: [{ modelId: "llama3.1-8b" }],
      priority: 8, // Fallback
    },
  ],
  autoMode: true, // B·∫≠t auto mode
});

// Kh√¥ng c·∫ßn ch·ªâ ƒë·ªãnh provider/model
const response = await aio.chatCompletion({
  messages: [
    { role: "user", content: "Hello!" },
  ],
});

// AIO t·ª± ƒë·ªông ch·ªçn Groq tr∆∞·ªõc, n·∫øu fail s·∫Ω fallback sang Cerebras
```

### 3. Priority Management

```typescript
const aio = new AIO({
  providers: [
    {
      provider: "groq",
      apiKeys: [
        { key: "gsk_primary", priority: 100 }, // Key ch√≠nh
        { key: "gsk_backup1", priority: 50 },  // Backup 1
        { key: "gsk_backup2", priority: 10 },  // Backup 2
      ],
      models: [
        { modelId: "llama-3.3-70b-versatile", priority: 100 }, // Model t·ªët nh·∫•t
        { modelId: "llama-3.1-8b-instant", priority: 50 },     // Model nhanh h∆°n
      ],
      priority: 100, // Provider priority
    },
  ],
  autoMode: true,
});

// AIO s·∫Ω th·ª≠ theo th·ª© t·ª±:
// 1. groq:llama-3.3-70b-versatile v·ªõi gsk_primary
// 2. N·∫øu fail ‚Üí th·ª≠ gsk_backup1
// 3. N·∫øu fail ‚Üí th·ª≠ gsk_backup2
// 4. N·∫øu fail ‚Üí th·ª≠ groq:llama-3.1-8b-instant
```

### 4. Streaming

```typescript
for await (const chunk of aio.chatCompletionStream({
  provider: "groq",
  modelId: "llama-3.3-70b-versatile",
  messages: [
    { role: "user", content: "Write a poem" },
  ],
})) {
  if (!chunk.done) {
    process.stdout.write(chunk.content);
  }
}
```

## üìö API Reference

### `AIO` Class

#### Constructor

```typescript
new AIO(config: AIOConfig)
```

#### Methods

- `chatCompletion(request: ChatCompletionRequest): Promise<ChatCompletionResponse>`
- `chatCompletionStream(request: ChatCompletionRequest): AsyncGenerator<StreamChunk>`
- `validateApiKey(provider: Provider, apiKey: string): Promise<boolean>`

### Types

#### `AIOConfig`

```typescript
interface AIOConfig {
  providers: ProviderConfig[];
  autoMode?: boolean;        // Default: false
  maxRetries?: number;       // Default: 3
  retryDelay?: number;       // Default: 1000ms
}
```

#### `ProviderConfig`

```typescript
interface ProviderConfig {
  provider: Provider;        // "openrouter" | "groq" | "cerebras" | "google-ai"
  apiKeys: ApiKey[];
  models: ModelConfig[];
  priority?: number;         // Default: 0 (cao h∆°n = ∆∞u ti√™n h∆°n)
  isActive?: boolean;        // Default: true
}
```

#### `ApiKey`

```typescript
interface ApiKey {
  key: string;
  priority?: number;         // Default: 0
  isActive?: boolean;        // Default: true
  dailyLimit?: number;
  requestsToday?: number;
}
```

#### `ModelConfig`

```typescript
interface ModelConfig {
  modelId: string;
  priority?: number;         // Default: 0
  isActive?: boolean;        // Default: true
}
```

#### `ChatCompletionRequest`

```typescript
interface ChatCompletionRequest {
  messages: Message[];
  temperature?: number;
  maxTokens?: number;
  
  // Direct mode
  provider?: Provider;
  modelId?: string;
}
```

## üéØ Supported Providers

| Provider | Base URL | Models |
|----------|----------|--------|
| OpenRouter | https://openrouter.ai/api/v1 | 30+ free models |
| Groq | https://api.groq.com/openai/v1 | llama-3.3-70b, llama-3.1-8b, etc. |
| Cerebras | https://api.cerebras.ai/v1 | llama3.1-8b, llama3.1-70b |
| Google AI | https://generativelanguage.googleapis.com | gemini-1.5-flash, gemini-1.5-pro |

## üìñ Examples

Xem th√™m examples trong th∆∞ m·ª•c `examples/`:

- `basic.ts` - Basic usage v·ªõi direct mode
- `auto-mode.ts` - Auto mode v·ªõi fallback
- `priority.ts` - Priority management
- `streaming.ts` - Streaming responses

Ch·∫°y examples:

```bash
npm run example:basic
npm run example:auto
npm run example:priority
```

## üõ†Ô∏è Development

```bash
# Install dependencies
npm install

# Build
npm run build

# Run examples
npm run dev
```

## üìÑ License

MIT

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.
