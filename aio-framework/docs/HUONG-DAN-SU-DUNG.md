# üìö H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG AIO-LLM FRAMEWORK

## üìñ M·ª•c l·ª•c

1. [Gi·ªõi thi·ªáu](#gi·ªõi-thi·ªáu)
2. [C√†i ƒë·∫∑t](#c√†i-ƒë·∫∑t)
3. [Kh·ªüi t·∫°o c∆° b·∫£n](#kh·ªüi-t·∫°o-c∆°-b·∫£n)
4. [C√°c ch·∫ø ƒë·ªô ho·∫°t ƒë·ªông](#c√°c-ch·∫ø-ƒë·ªô-ho·∫°t-ƒë·ªông)
5. [T√≠nh nƒÉng n√¢ng cao](#t√≠nh-nƒÉng-n√¢ng-cao)
6. [API Reference](#api-reference)
7. [Examples](#examples)
8. [Troubleshooting](#troubleshooting)

---

## üéØ Gi·ªõi thi·ªáu

**AIO-LLM** (All-In-One LLM Framework) l√† m·ªôt framework TypeScript/JavaScript m·∫°nh m·∫Ω gi√∫p b·∫°n t√≠ch h·ª£p nhi·ªÅu nh√† cung c·∫•p LLM (Large Language Model) m·ªôt c√°ch d·ªÖ d√†ng v·ªõi c√°c t√≠nh nƒÉng:

### ‚ú® T√≠nh nƒÉng ch√≠nh

- **Multi-Provider Support**: H·ªó tr·ª£ 4 providers ph·ªï bi·∫øn
  - OpenRouter (30+ free models)
  - Groq (llama-3.3-70b, llama-3.1-8b, etc.)
  - Cerebras (llama3.1-8b, llama3.1-70b)
  - Google AI (gemini-1.5-flash, gemini-1.5-pro)

- **Auto Fallback**: T·ª± ƒë·ªông chuy·ªÉn sang provider/model kh√°c khi g·∫∑p l·ªói
- **Priority Management**: Qu·∫£n l√Ω ƒë·ªô ∆∞u ti√™n cho providers, models v√† API keys
- **Key Rotation**: T·ª± ƒë·ªông th·ª≠ c√°c API keys kh√°c khi key hi·ªán t·∫°i fail
- **Multimodal Support**: H·ªó tr·ª£ images, video, audio, PDF (Google AI)
- **Structured Outputs**: JSON mode v√† JSON Schema validation
- **Streaming**: Real-time streaming responses
- **Abort Control**: Cancel requests b·∫•t k·ª≥ l√∫c n√†o
- **Retry Logic**: Exponential backoff retry v·ªõi error classification
- **Validation**: Zod schema validation cho config v√† requests
- **Logging**: Winston logger v·ªõi multiple levels

---

## üì¶ C√†i ƒë·∫∑t

```bash
npm install aio-llm
```

### Dependencies

Framework s·ª≠ d·ª•ng c√°c dependencies sau:

```json
{
  "@google/genai": "^1.34.0",
  "groq-sdk": "^0.37.0",
  "openai": "^6.15.0",
  "winston": "^3.19.0",
  "zod": "^4.3.6"
}
```

---

## üöÄ Kh·ªüi t·∫°o c∆° b·∫£n

### 1. Chu·∫©n b·ªã API Keys

T·∫°o file `.env` trong th∆∞ m·ª•c g·ªëc c·ªßa project:

```env
# OpenRouter
OPENROUTER_API_KEY=sk-or-v1-xxxxx

# Groq
GROQ_API_KEY=gsk_xxxxx

# Cerebras
CEREBRAS_API_KEY=csk_xxxxx

# Google AI
GOOGLE_AI_API_KEY=AIzaSyxxxxx
```

### 2. Import v√† kh·ªüi t·∫°o

```typescript
import { AIO } from "aio-llm";
import dotenv from "dotenv";

dotenv.config();

const aio = new AIO({
  providers: [
    {
      provider: "openrouter",
      apiKeys: [{ key: process.env.OPENROUTER_API_KEY }],
      models: [{ modelId: "arcee-ai/trinity-large-preview:free" }],
    },
  ],
});
```

### 3. G·ª≠i request ƒë·∫ßu ti√™n

```typescript
const response = await aio.chatCompletion({
  provider: "openrouter",
  model: "arcee-ai/trinity-large-preview:free",
  messages: [
    { role: "user", content: "Xin ch√†o! B·∫°n l√† ai?" }
  ],
});

console.log(response.choices[0].message.content);
```

---

## üéÆ C√°c ch·∫ø ƒë·ªô ho·∫°t ƒë·ªông

### 1. Direct Mode (Ch·∫ø ƒë·ªô ch·ªâ ƒë·ªãnh c·ª• th·ªÉ)

Ch·ªâ ƒë·ªãnh r√µ provider v√† model b·∫°n mu·ªën s·ª≠ d·ª•ng.

```typescript
const aio = new AIO({
  providers: [
    {
      provider: "groq",
      apiKeys: [{ key: process.env.GROQ_API_KEY }],
      models: [{ modelId: "llama-3.3-70b-versatile" }],
    },
  ],
  autoMode: false, // M·∫∑c ƒë·ªãnh l√† false
});

// Ph·∫£i ch·ªâ ƒë·ªãnh provider v√† model
const response = await aio.chatCompletion({
  provider: "groq",
  model: "llama-3.3-70b-versatile",
  messages: [{ role: "user", content: "Hello!" }],
});
```

**∆Øu ƒëi·ªÉm:**
- Ki·ªÉm so√°t ho√†n to√†n provider/model ƒë∆∞·ª£c s·ª≠ d·ª•ng
- Ph√π h·ª£p khi b·∫°n mu·ªën test m·ªôt model c·ª• th·ªÉ

**Nh∆∞·ª£c ƒëi·ªÉm:**
- Kh√¥ng c√≥ fallback t·ª± ƒë·ªông
- Ph·∫£i handle errors manually

### 2. Auto Mode (Ch·∫ø ƒë·ªô t·ª± ƒë·ªông)

Framework t·ª± ƒë·ªông ch·ªçn provider/model theo priority v√† fallback khi fail.

```typescript
const aio = new AIO({
  providers: [
    {
      provider: "groq",
      apiKeys: [{ key: process.env.GROQ_API_KEY }],
      models: [
        { modelId: "llama-3.3-70b-versatile", priority: 100 },
        { modelId: "llama-3.1-8b-instant", priority: 50 },
      ],
      priority: 100, // Groq ∆∞u ti√™n cao nh·∫•t
    },
    {
      provider: "cerebras",
      apiKeys: [{ key: process.env.CEREBRAS_API_KEY }],
      models: [{ modelId: "llama3.1-8b", priority: 80 }],
      priority: 80, // Cerebras l√† fallback
    },
  ],
  autoMode: true, // B·∫≠t auto mode
});

// Kh√¥ng c·∫ßn ch·ªâ ƒë·ªãnh provider/model
const response = await aio.chatCompletion({
  messages: [{ role: "user", content: "Hello!" }],
});

// Ki·ªÉm tra c√≥ fallback kh√¥ng
if (response.auto_fallback) {
  console.log("ƒê√£ fallback t·ª´:", response.auto_fallback.original_provider);
  console.log("Sang:", response.auto_fallback.final_provider);
}
```

**∆Øu ƒëi·ªÉm:**
- T·ª± ƒë·ªông fallback khi provider/model fail
- T·ªëi ∆∞u reliability v√† uptime
- Kh√¥ng c·∫ßn handle errors ph·ª©c t·∫°p

**Nh∆∞·ª£c ƒëi·ªÉm:**
- √çt ki·ªÉm so√°t h∆°n
- C√≥ th·ªÉ t·ªën th·ªùi gian n·∫øu nhi·ªÅu fallback

---

## üîë Priority Management

### C√°ch ho·∫°t ƒë·ªông c·ªßa Priority

Priority c√†ng cao = ∆∞u ti√™n c√†ng cao (s·ªë l·ªõn h∆°n ƒë∆∞·ª£c th·ª≠ tr∆∞·ªõc).

Framework c√≥ 3 c·∫•p ƒë·ªô priority:

1. **Provider Priority**: Ch·ªçn provider n√†o tr∆∞·ªõc
2. **Model Priority**: Trong c√πng provider, ch·ªçn model n√†o tr∆∞·ªõc
3. **API Key Priority**: Trong c√πng provider, ch·ªçn key n√†o tr∆∞·ªõc

### V√≠ d·ª• Priority ƒë·∫ßy ƒë·ªß

```typescript
const aio = new AIO({
  providers: [
    {
      provider: "groq",
      priority: 100, // Provider priority cao nh·∫•t
      apiKeys: [
        { key: "gsk_primary", priority: 100 },   // Key ch√≠nh
        { key: "gsk_backup1", priority: 50 },    // Backup 1
        { key: "gsk_backup2", priority: 10 },    // Backup 2
      ],
      models: [
        { modelId: "llama-3.3-70b-versatile", priority: 100 }, // Model t·ªët nh·∫•t
        { modelId: "llama-3.1-8b-instant", priority: 50 },     // Model nhanh h∆°n
      ],
    },
    {
      provider: "cerebras",
      priority: 80, // Fallback provider
      apiKeys: [{ key: "csk_key", priority: 100 }],
      models: [{ modelId: "llama3.1-8b", priority: 100 }],
    },
  ],
  autoMode: true,
});
```

**Th·ª© t·ª± th·ª≠:**
1. groq:llama-3.3-70b-versatile v·ªõi gsk_primary
2. N·∫øu fail ‚Üí th·ª≠ gsk_backup1
3. N·∫øu fail ‚Üí th·ª≠ gsk_backup2
4. N·∫øu fail ‚Üí th·ª≠ groq:llama-3.1-8b-instant
5. N·∫øu fail ‚Üí th·ª≠ cerebras:llama3.1-8b

---

## üé® T√≠nh nƒÉng n√¢ng cao

### 1. System Prompt

Th√™m system prompt ƒë·ªÉ ƒë·ªãnh h∆∞·ªõng behavior c·ªßa AI.

```typescript
const response = await aio.chatCompletion({
  provider: "openrouter",
  model: "arcee-ai/trinity-large-preview:free",
  systemPrompt: "B·∫°n l√† m·ªôt chuy√™n gia l·∫≠p tr√¨nh Python. Lu√¥n tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.",
  messages: [
    { role: "user", content: "Gi·∫£i th√≠ch list comprehension" }
  ],
});
```

**L∆∞u √Ω:**
- OpenRouter, Groq, Cerebras: System prompt ƒë∆∞·ª£c th√™m v√†o messages array
- Google AI: System prompt ƒë∆∞·ª£c g·ª≠i qua `systemInstruction` parameter

### 2. Temperature v√† Sampling Parameters

ƒêi·ªÅu ch·ªânh t√≠nh s√°ng t·∫°o v√† ƒëa d·∫°ng c·ªßa response.

```typescript
const response = await aio.chatCompletion({
  provider: "google-ai",
  model: "gemini-1.5-flash",
  messages: [{ role: "user", content: "Vi·∫øt m·ªôt c√¢u chuy·ªán ng·∫Øn" }],
  temperature: 0.9,    // 0.0-2.0, cao = s√°ng t·∫°o h∆°n
  max_tokens: 500,     // Gi·ªõi h·∫°n ƒë·ªô d√†i response
  top_p: 0.95,         // Nucleus sampling (0.0-1.0)
  top_k: 40,           // Top-K sampling (ch·ªâ Google AI v√† OpenRouter)
  stop: ["END", "---"], // Stop sequences
});
```

**Gi·∫£i th√≠ch parameters:**

- **temperature**: ƒê·ªô "s√°ng t·∫°o"
  - 0.0-0.3: Deterministic, consistent (code, facts)
  - 0.4-0.7: Balanced (general chat)
  - 0.8-2.0: Creative, diverse (stories, brainstorming)

- **max_tokens**: S·ªë tokens t·ªëi ƒëa trong response
  - T√≠nh c·∫£ input + output
  - M·ªói provider c√≥ gi·ªõi h·∫°n kh√°c nhau

- **top_p**: Nucleus sampling
  - 0.9-1.0: ƒêa d·∫°ng h∆°n
  - 0.1-0.5: T·∫≠p trung h∆°n

- **top_k**: Top-K sampling (ch·ªâ Google AI v√† OpenRouter)
  - Ch·ªçn t·ª´ top K tokens c√≥ x√°c su·∫•t cao nh·∫•t
  - 1-10: R·∫•t t·∫≠p trung
  - 40-100: C√¢n b·∫±ng

### 3. Streaming Responses

Nh·∫≠n response theo real-time thay v√¨ ƒë·ª£i ho√†n th√†nh.

```typescript
const stream = await aio.chatCompletionStream({
  provider: "groq",
  model: "llama-3.3-70b-versatile",
  messages: [
    { role: "user", content: "Vi·∫øt m·ªôt b√†i th∆° v·ªÅ m√πa thu" }
  ],
});

// C√°ch 1: S·ª≠ d·ª•ng event listeners
stream.on("data", (chunk) => {
  const text = chunk.toString();
  const lines = text.split("\n");
  
  for (const line of lines) {
    if (line.startsWith("data: ") && !line.includes("[DONE]")) {
      try {
        const data = JSON.parse(line.slice(6));
        const content = data.choices?.[0]?.delta?.content;
        if (content) {
          process.stdout.write(content);
        }
      } catch (e) {
        // Skip invalid JSON
      }
    }
  }
});

stream.on("end", () => {
  console.log("\n‚úÖ Ho√†n th√†nh!");
});

stream.on("error", (error) => {
  console.error("‚ùå L·ªói:", error);
});

// C√°ch 2: S·ª≠ d·ª•ng for await...of
for await (const chunk of stream) {
  // X·ª≠ l√Ω chunk t∆∞∆°ng t·ª± nh∆∞ tr√™n
}
```

**∆Øu ƒëi·ªÉm c·ªßa Streaming:**
- User experience t·ªët h∆°n (th·∫•y response ngay l·∫≠p t·ª©c)
- Ph√π h·ª£p v·ªõi long-form content
- C√≥ th·ªÉ cancel gi·ªØa ch·ª´ng

### 4. Abort/Cancel Requests

Cancel request ƒëang ch·∫°y b·∫•t k·ª≥ l√∫c n√†o.

#### Cancel Non-Streaming Request

```typescript
const controller = new AbortController();

// Cancel sau 5 gi√¢y
setTimeout(() => {
  controller.abort();
  console.log("‚è±Ô∏è ƒê√£ cancel request");
}, 5000);

try {
  const response = await aio.chatCompletion({
    provider: "openrouter",
    model: "arcee-ai/trinity-large-preview:free",
    messages: [{ role: "user", content: "Vi·∫øt m·ªôt b√†i lu·∫≠n d√†i..." }],
    signal: controller.signal, // Truy·ªÅn abort signal
  });
} catch (error) {
  if (error.message.includes("cancel")) {
    console.log("‚úÖ Request ƒë√£ b·ªã cancel th√†nh c√¥ng");
  }
}
```

#### Cancel Streaming Request

```typescript
const controller = new AbortController();

const stream = await aio.chatCompletionStream({
  provider: "groq",
  model: "llama-3.3-70b-versatile",
  messages: [{ role: "user", content: "ƒê·∫øm t·ª´ 1 ƒë·∫øn 1000" }],
  signal: controller.signal,
});

let chunks = 0;
for await (const chunk of stream) {
  chunks++;
  console.log(`Chunk ${chunks}`);
  
  if (chunks >= 10) {
    controller.abort(); // Cancel sau 10 chunks
    break;
  }
}
```

#### Pre-cancelled Request

```typescript
const controller = new AbortController();
controller.abort(); // Cancel tr∆∞·ªõc khi g·ªçi

try {
  await aio.chatCompletion({
    provider: "openrouter",
    model: "arcee-ai/trinity-large-preview:free",
    messages: [{ role: "user", content: "Test" }],
    signal: controller.signal,
  });
} catch (error) {
  console.log("Request ƒë√£ b·ªã cancel tr∆∞·ªõc khi th·ª±c thi");
}
```

### 5. Multimodal Input (Ch·ªâ Google AI)

G·ª≠i images, video, audio, PDF c√πng v·ªõi text.

#### Image t·ª´ Base64

```typescript
import fs from "fs";

// ƒê·ªçc image v√† convert sang base64
const imageBuffer = fs.readFileSync("./image.jpg");
const base64Image = imageBuffer.toString("base64");

const response = await aio.chatCompletion({
  provider: "google-ai",
  model: "gemini-1.5-flash",
  messages: [
    {
      role: "user",
      content: [
        { type: "text", text: "M√¥ t·∫£ h√¨nh ·∫£nh n√†y" },
        {
          type: "image",
          source: {
            type: "base64",
            media_type: "image/jpeg",
            data: base64Image,
          },
        },
      ],
    },
  ],
});
```

#### Image t·ª´ URL

```typescript
const response = await aio.chatCompletion({
  provider: "google-ai",
  model: "gemini-1.5-flash",
  messages: [
    {
      role: "user",
      content: [
        { type: "text", text: "C√≥ g√¨ trong h√¨nh n√†y?" },
        {
          type: "image",
          source: {
            type: "url",
            media_type: "image/jpeg",
            url: "https://example.com/image.jpg",
          },
        },
      ],
    },
  ],
});
```

#### Video, Audio, PDF

```typescript
// Video
const response = await aio.chatCompletion({
  provider: "google-ai",
  model: "gemini-1.5-flash",
  messages: [
    {
      role: "user",
      content: [
        { type: "text", text: "T√≥m t·∫Øt video n√†y" },
        {
          type: "file",
          source: {
            type: "base64",
            media_type: "video/mp4",
            data: base64VideoData,
          },
        },
      ],
    },
  ],
});

// Audio
const response2 = await aio.chatCompletion({
  provider: "google-ai",
  model: "gemini-1.5-flash",
  messages: [
    {
      role: "user",
      content: [
        { type: "text", text: "Transcribe audio n√†y" },
        {
          type: "file",
          source: {
            type: "base64",
            media_type: "audio/mp3",
            data: base64AudioData,
          },
        },
      ],
    },
  ],
});

// PDF
const response3 = await aio.chatCompletion({
  provider: "google-ai",
  model: "gemini-1.5-flash",
  messages: [
    {
      role: "user",
      content: [
        { type: "text", text: "T√≥m t·∫Øt t√†i li·ªáu PDF n√†y" },
        {
          type: "file",
          source: {
            type: "base64",
            media_type: "application/pdf",
            data: base64PdfData,
          },
        },
      ],
    },
  ],
});
```

**Supported MIME types:**
- Images: `image/jpeg`, `image/png`, `image/webp`, `image/gif`
- Video: `video/mp4`, `video/mpeg`, `video/mov`, `video/avi`, `video/webm`
- Audio: `audio/mp3`, `audio/wav`, `audio/aac`, `audio/ogg`
- Documents: `application/pdf`

---

### 6. Structured Outputs (JSON Mode)

B·∫Øt bu·ªôc AI tr·∫£ v·ªÅ JSON format.

#### JSON Object Mode

Tr·∫£ v·ªÅ valid JSON nh∆∞ng kh√¥ng c√≥ schema c·ª• th·ªÉ.

```typescript
const response = await aio.chatCompletion({
  provider: "openrouter",
  model: "arcee-ai/trinity-large-preview:free",
  messages: [
    {
      role: "user",
      content: "Tr·∫£ v·ªÅ th√¥ng tin: T√™n: Nguy·ªÖn VƒÉn A, Tu·ªïi: 25, Th√†nh ph·ªë: H√† N·ªôi",
    },
  ],
  response_format: { type: "json_object" },
});

const data = JSON.parse(response.choices[0].message.content);
console.log(data);
// { "name": "Nguy·ªÖn VƒÉn A", "age": 25, "city": "H√† N·ªôi" }
```

**L∆∞u √Ω:**
- Ph·∫£i nh·∫Øc AI tr·∫£ v·ªÅ JSON trong prompt
- Kh√¥ng ƒë·∫£m b·∫£o schema c·ª• th·ªÉ
- H·ªó tr·ª£: OpenRouter, Groq, Cerebras, Google AI

#### JSON Schema Mode

Tr·∫£ v·ªÅ JSON theo schema c·ª• th·ªÉ (structured outputs).

```typescript
const response = await aio.chatCompletion({
  provider: "openrouter",
  model: "arcee-ai/trinity-large-preview:free",
  messages: [
    {
      role: "user",
      content: "Ph√¢n t√≠ch review: iPhone 15 Pro - Camera tuy·ªát v·ªùi, gi√° h∆°i cao. Rating: 4.5/5",
    },
  ],
  response_format: {
    type: "json_schema",
    json_schema: {
      name: "product_review",
      strict: true, // B·∫Øt bu·ªôc tu√¢n th·ªß schema
      schema: {
        type: "object",
        properties: {
          product_name: { type: "string" },
          rating: { type: "number" },
          sentiment: {
            type: "string",
            enum: ["positive", "negative", "neutral"],
          },
          key_features: {
            type: "array",
            items: { type: "string" },
          },
          price_opinion: { type: "string" },
        },
        required: ["product_name", "rating", "sentiment", "key_features"],
        additionalProperties: false,
      },
    },
  },
});

const data = JSON.parse(response.choices[0].message.content);
console.log(data);
// {
//   "product_name": "iPhone 15 Pro",
//   "rating": 4.5,
//   "sentiment": "positive",
//   "key_features": ["Camera tuy·ªát v·ªùi"],
//   "price_opinion": "Gi√° h∆°i cao"
// }
```

**∆Øu ƒëi·ªÉm:**
- ƒê·∫£m b·∫£o 100% tu√¢n th·ªß schema
- Kh√¥ng c·∫ßn validate response
- Type-safe khi parse

**Use cases:**
- Extract structured data t·ª´ text
- Form filling
- Data transformation
- API responses

**H·ªó tr·ª£:**
- OpenRouter: ‚úÖ (strict mode)
- Groq: ‚úÖ (strict mode)
- Cerebras: ‚úÖ (strict mode)
- Google AI: ‚úÖ (responseSchema)

### 7. Conversation Memory

Duy tr√¨ context qua nhi·ªÅu messages.

```typescript
const messages = [
  { role: "user", content: "T√™n t√¥i l√† Minh" },
];

// Turn 1
let response = await aio.chatCompletion({
  provider: "openrouter",
  model: "arcee-ai/trinity-large-preview:free",
  messages,
});

messages.push({
  role: "assistant",
  content: response.choices[0].message.content,
});

// Turn 2
messages.push({
  role: "user",
  content: "T√™n t√¥i l√† g√¨?",
});

response = await aio.chatCompletion({
  provider: "openrouter",
  model: "arcee-ai/trinity-large-preview:free",
  messages,
});

console.log(response.choices[0].message.content);
// "T√™n b·∫°n l√† Minh"
```

**Best practices:**
- L∆∞u to√†n b·ªô conversation history
- Gi·ªõi h·∫°n s·ªë messages ƒë·ªÉ tr√°nh v∆∞·ª£t token limit
- C√≥ th·ªÉ summarize old messages ƒë·ªÉ ti·∫øt ki·ªám tokens

### 8. Key Management

Qu·∫£n l√Ω API keys v·ªõi daily limits v√† error tracking.

#### Daily Limits

```typescript
const aio = new AIO({
  providers: [
    {
      provider: "openrouter",
      apiKeys: [
        {
          key: process.env.OPENROUTER_API_KEY,
          priority: 10,
          dailyLimit: 100, // Gi·ªõi h·∫°n 100 requests/ng√†y
          requestsToday: 0, // S·ªë requests ƒë√£ d√πng h√¥m nay
        },
      ],
      models: [{ modelId: "arcee-ai/trinity-large-preview:free" }],
    },
  ],
});

// Sau m·ªói request, requestsToday t·ª± ƒë·ªông tƒÉng
// Khi ƒë·∫°t dailyLimit, key s·∫Ω kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng n·ªØa

// Reset daily counters (g·ªçi m·ªói ng√†y)
aio.resetDailyCounters();
```

#### Key Statistics

```typescript
// L·∫•y th·ªëng k√™ keys c·ªßa m·ªôt provider
const stats = aio.getKeyStats("openrouter");
console.log(stats);
// {
//   total: 3,           // T·ªïng s·ªë keys
//   active: 2,          // S·ªë keys ƒëang active
//   disabled: 1,        // S·ªë keys b·ªã disabled
//   totalUsage: 150,    // T·ªïng s·ªë requests ƒë√£ d√πng
//   totalErrors: 5      // T·ªïng s·ªë errors
// }
```

#### Config Summary

```typescript
const summary = aio.getConfigSummary();
console.log(summary);
// {
//   providers: 2,       // S·ªë providers
//   totalKeys: 5,       // T·ªïng s·ªë API keys
//   totalModels: 8,     // T·ªïng s·ªë models
//   autoMode: true,     // Auto mode enabled?
//   maxRetries: 3       // Max retry attempts
// }
```

### 9. Error Handling v√† Retry Logic

Framework t·ª± ƒë·ªông classify errors v√† retry khi c·∫ßn.

#### Error Classification

```typescript
import { AIOError } from "aio-llm";

try {
  const response = await aio.chatCompletion({
    provider: "openrouter",
    model: "arcee-ai/trinity-large-preview:free",
    messages: [{ role: "user", content: "Hello" }],
  });
} catch (error) {
  if (error instanceof AIOError) {
    const classification = AIOError.classify(error);
    
    console.log("Category:", classification.category);
    // "rate_limit" | "auth" | "invalid_request" | "server" | "network" | "unknown"
    
    console.log("Is Retryable:", classification.isRetryable);
    // true/false
    
    console.log("Should Rotate Key:", classification.shouldRotateKey);
    // true/false
  }
}
```

**Error Categories:**

| Category | Retryable | Rotate Key | Examples |
|----------|-----------|------------|----------|
| `rate_limit` | ‚úÖ | ‚úÖ | Rate limit exceeded, 429 |
| `auth` | ‚ùå | ‚úÖ | Invalid API key, 401, 403 |
| `invalid_request` | ‚ùå | ‚ùå | Bad request, 400 |
| `server` | ‚úÖ | ‚ùå | 500, 502, 503, 504 |
| `network` | ‚úÖ | ‚ùå | Timeout, ECONNRESET |
| `unknown` | ‚ùå | ‚ùå | Other errors |

#### Retry Configuration

```typescript
const aio = new AIO({
  providers: [
    {
      provider: "openrouter",
      apiKeys: [{ key: process.env.OPENROUTER_API_KEY }],
      models: [{ modelId: "arcee-ai/trinity-large-preview:free" }],
    },
  ],
  maxRetries: 5,        // S·ªë l·∫ßn retry t·ªëi ƒëa (default: 3)
  retryDelay: 2000,     // Delay gi·ªØa c√°c retry (ms) (default: 1000)
});
```

**Retry Logic:**
- Exponential backoff: delay √ó 2^(attempt-1)
- Ch·ªâ retry v·ªõi retryable errors
- T·ª± ƒë·ªông rotate key n·∫øu c·∫ßn

#### Custom Error Handling

```typescript
try {
  const response = await aio.chatCompletion({
    provider: "openrouter",
    model: "arcee-ai/trinity-large-preview:free",
    messages: [{ role: "user", content: "Hello" }],
  });
} catch (error) {
  if (error instanceof AIOError) {
    console.error("Provider:", error.provider);
    console.error("Model:", error.model);
    console.error("Status Code:", error.statusCode);
    console.error("Message:", error.message);
    
    // Handle specific errors
    if (error.statusCode === 429) {
      console.log("Rate limit - ƒë·ª£i 1 ph√∫t r·ªìi th·ª≠ l·∫°i");
    } else if (error.statusCode === 401) {
      console.log("API key kh√¥ng h·ª£p l·ªá");
    }
  }
}
```

### 10. Logging

Framework s·ª≠ d·ª•ng Winston logger v·ªõi multiple levels.

#### Enable/Disable Logging

```typescript
const aio = new AIO({
  providers: [...],
  enableLogging: true, // Default: true
});
```

#### Log Levels

- **error**: Critical errors
- **warn**: Warnings (retry attempts, key rotation)
- **info**: General info (requests, responses)
- **debug**: Detailed debug info (key usage, etc.)

#### Custom Logger

```typescript
import { logger } from "aio-llm";

// Thay ƒë·ªïi log level
logger.level = "debug"; // "error" | "warn" | "info" | "debug"

// Custom log
logger.info("Custom message", { key: "value" });
```

---

## üìñ API Reference

### AIO Class

#### Constructor

```typescript
new AIO(config: AIOConfig)
```

**Parameters:**

```typescript
interface AIOConfig {
  providers: ProviderConfig[];      // Danh s√°ch providers
  autoMode?: boolean;                // Auto fallback mode (default: false)
  maxRetries?: number;               // Max retry attempts (default: 3)
  retryDelay?: number;               // Delay between retries (ms) (default: 1000)
  enableLogging?: boolean;           // Enable Winston logging (default: true)
  enableValidation?: boolean;        // Enable Zod validation (default: true)
}
```

#### Methods

##### chatCompletion()

```typescript
async chatCompletion(request: ChatCompletionRequest): Promise<ChatCompletionResponse>
```

G·ª≠i chat completion request (non-streaming).

**Parameters:**

```typescript
interface ChatCompletionRequest {
  messages: Message[];               // Conversation messages
  systemPrompt?: string;             // System prompt
  temperature?: number;              // 0.0-2.0 (default: 1.0)
  max_tokens?: number;               // Max output tokens
  top_p?: number;                    // Nucleus sampling (0.0-1.0)
  top_k?: number;                    // Top-K sampling (Google AI, OpenRouter)
  stop?: string[];                   // Stop sequences
  response_format?: ResponseFormat;  // JSON mode/schema
  provider?: Provider;               // Provider (required in direct mode)
  model?: string;                    // Model (required in direct mode)
  signal?: AbortSignal;              // Abort signal
}
```

**Returns:**

```typescript
interface ChatCompletionResponse {
  id: string;                        // Response ID
  provider: Provider;                // Provider used
  model: string;                     // Model used
  choices: {
    index: number;
    message: Message;
    finish_reason: string;
  }[];
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
  created: number;                   // Timestamp
  auto_fallback?: {                  // Fallback info (if occurred)
    original_provider: string;
    original_model: string;
    final_provider: string;
    final_model: string;
    fallback_count: number;
  };
}
```

##### chatCompletionStream()

```typescript
async chatCompletionStream(request: ChatCompletionRequest): Promise<Readable>
```

G·ª≠i chat completion request (streaming).

**Returns:** Node.js Readable stream

**Stream format:**

```
data: {"id":"...","provider":"...","model":"...","choices":[{"index":0,"delta":{"content":"Hello"},"finish_reason":null}]}

data: {"id":"...","provider":"...","model":"...","choices":[{"index":0,"delta":{"content":" world"},"finish_reason":null}]}

data: [DONE]
```

##### getKeyStats()

```typescript
getKeyStats(provider: Provider): KeyStats | null
```

L·∫•y th·ªëng k√™ keys c·ªßa m·ªôt provider.

**Returns:**

```typescript
interface KeyStats {
  total: number;          // T·ªïng s·ªë keys
  active: number;         // S·ªë keys active
  disabled: number;       // S·ªë keys disabled
  totalUsage: number;     // T·ªïng requests ƒë√£ d√πng
  totalErrors: number;    // T·ªïng errors
}
```

##### resetDailyCounters()

```typescript
resetDailyCounters(): void
```

Reset daily counters cho t·∫•t c·∫£ keys (g·ªçi m·ªói ng√†y).

##### getConfigSummary()

```typescript
getConfigSummary(): ConfigSummary
```

L·∫•y t√≥m t·∫Øt configuration.

**Returns:**

```typescript
interface ConfigSummary {
  providers: number;      // S·ªë providers
  totalKeys: number;      // T·ªïng s·ªë keys
  totalModels: number;    // T·ªïng s·ªë models
  autoMode: boolean;      // Auto mode enabled?
  maxRetries: number;     // Max retry attempts
}
```

### Types

#### Provider

```typescript
type Provider = "openrouter" | "groq" | "cerebras" | "google-ai";
```

#### Message

```typescript
interface Message {
  role: "system" | "user" | "assistant";
  content: string | MessageContent[];
}

type MessageContent = TextContent | ImageContent | FileContent;

interface TextContent {
  type: "text";
  text: string;
}

interface ImageContent {
  type: "image";
  source: {
    type: "base64" | "url";
    media_type: string;
    data?: string;
    url?: string;
  };
}

interface FileContent {
  type: "file";
  source: {
    type: "base64" | "url";
    media_type: string;
    data?: string;
    url?: string;
  };
}
```

#### ResponseFormat

```typescript
type ResponseFormat =
  | { type: "text" }                    // Plain text (default)
  | { type: "json_object" }             // Valid JSON
  | {
      type: "json_schema";              // Structured outputs
      json_schema: {
        name: string;
        strict?: boolean;
        schema: Record<string, any>;    // JSON Schema
        description?: string;
      };
    };
```

#### ProviderConfig

```typescript
interface ProviderConfig {
  provider: Provider;
  apiKeys: ApiKey[];
  models: ModelConfig[];
  priority?: number;        // Default: 0
  isActive?: boolean;       // Default: true
}
```

#### ApiKey

```typescript
interface ApiKey {
  key: string;
  priority?: number;        // Default: 0
  isActive?: boolean;       // Default: true
  dailyLimit?: number;      // Max requests/day
  requestsToday?: number;   // Current usage
  errorCount?: number;      // Consecutive errors
  lastError?: string;       // Last error message
  lastUsed?: Date;          // Last usage timestamp
}
```

#### ModelConfig

```typescript
interface ModelConfig {
  modelId: string;
  priority?: number;        // Default: 0
  isActive?: boolean;       // Default: true
}
```

#### AIOError

```typescript
class AIOError extends Error {
  constructor(
    message: string,
    public provider?: Provider,
    public model?: string,
    public statusCode?: number,
    public isRetryable: boolean = false
  );
  
  static classify(error: any): {
    isRetryable: boolean;
    shouldRotateKey: boolean;
    category: "rate_limit" | "auth" | "invalid_request" | "server" | "network" | "unknown";
  };
}
```

---

## üí° Examples

### Example 1: Basic Chat

```typescript
import { AIO } from "aio-llm";

const aio = new AIO({
  providers: [
    {
      provider: "openrouter",
      apiKeys: [{ key: process.env.OPENROUTER_API_KEY }],
      models: [{ modelId: "arcee-ai/trinity-large-preview:free" }],
    },
  ],
});

const response = await aio.chatCompletion({
  provider: "openrouter",
  model: "arcee-ai/trinity-large-preview:free",
  messages: [
    { role: "user", content: "Gi·∫£i th√≠ch AI l√† g√¨?" }
  ],
});

console.log(response.choices[0].message.content);
```

### Example 2: Multi-Provider v·ªõi Auto Fallback

```typescript
const aio = new AIO({
  providers: [
    {
      provider: "groq",
      apiKeys: [{ key: process.env.GROQ_API_KEY }],
      models: [{ modelId: "llama-3.3-70b-versatile" }],
      priority: 100,
    },
    {
      provider: "cerebras",
      apiKeys: [{ key: process.env.CEREBRAS_API_KEY }],
      models: [{ modelId: "llama3.1-8b" }],
      priority: 80,
    },
    {
      provider: "google-ai",
      apiKeys: [{ key: process.env.GOOGLE_AI_API_KEY }],
      models: [{ modelId: "gemini-1.5-flash" }],
      priority: 60,
    },
  ],
  autoMode: true,
});

const response = await aio.chatCompletion({
  messages: [
    { role: "user", content: "Vi·∫øt code Python ƒë·ªÉ ƒë·ªçc file CSV" }
  ],
});

console.log(response.choices[0].message.content);
```

### Example 3: Streaming v·ªõi Cancel

```typescript
const controller = new AbortController();

// Cancel sau 10 gi√¢y
setTimeout(() => controller.abort(), 10000);

const stream = await aio.chatCompletionStream({
  provider: "groq",
  model: "llama-3.3-70b-versatile",
  messages: [
    { role: "user", content: "Vi·∫øt m·ªôt c√¢u chuy·ªán d√†i" }
  ],
  signal: controller.signal,
});

for await (const chunk of stream) {
  // Process chunk
}
```

### Example 4: JSON Schema Extraction

```typescript
const response = await aio.chatCompletion({
  provider: "openrouter",
  model: "arcee-ai/trinity-large-preview:free",
  messages: [
    {
      role: "user",
      content: "Extract info: John Doe, 30 tu·ªïi, Software Engineer t·∫°i Google, email: john@example.com",
    },
  ],
  response_format: {
    type: "json_schema",
    json_schema: {
      name: "person_info",
      strict: true,
      schema: {
        type: "object",
        properties: {
          name: { type: "string" },
          age: { type: "number" },
          job_title: { type: "string" },
          company: { type: "string" },
          email: { type: "string" },
        },
        required: ["name", "age", "job_title", "company", "email"],
        additionalProperties: false,
      },
    },
  },
});

const data = JSON.parse(response.choices[0].message.content);
console.log(data);
```

### Example 5: Image Analysis (Google AI)

```typescript
import fs from "fs";

const imageBuffer = fs.readFileSync("./photo.jpg");
const base64Image = imageBuffer.toString("base64");

const response = await aio.chatCompletion({
  provider: "google-ai",
  model: "gemini-1.5-flash",
  messages: [
    {
      role: "user",
      content: [
        { type: "text", text: "M√¥ t·∫£ chi ti·∫øt h√¨nh ·∫£nh n√†y" },
        {
          type: "image",
          source: {
            type: "base64",
            media_type: "image/jpeg",
            data: base64Image,
          },
        },
      ],
    },
  ],
});

console.log(response.choices[0].message.content);
```

### Example 6: Multi-Turn Conversation

```typescript
const messages = [];

// Turn 1
messages.push({
  role: "user",
  content: "T√¥i mu·ªën h·ªçc l·∫≠p tr√¨nh. N√™n b·∫Øt ƒë·∫ßu t·ª´ ƒë√¢u?",
});

let response = await aio.chatCompletion({
  provider: "openrouter",
  model: "arcee-ai/trinity-large-preview:free",
  messages,
});

messages.push({
  role: "assistant",
  content: response.choices[0].message.content,
});

// Turn 2
messages.push({
  role: "user",
  content: "Python hay JavaScript t·ªët h∆°n cho ng∆∞·ªùi m·ªõi?",
});

response = await aio.chatCompletion({
  provider: "openrouter",
  model: "arcee-ai/trinity-large-preview:free",
  messages,
});

console.log(response.choices[0].message.content);
```

---

## üîß Troubleshooting

### 1. "No API keys configured for provider"

**Nguy√™n nh√¢n:** Kh√¥ng c√≥ API key n√†o ƒë∆∞·ª£c c·∫•u h√¨nh cho provider.

**Gi·∫£i ph√°p:**

```typescript
const aio = new AIO({
  providers: [
    {
      provider: "openrouter",
      apiKeys: [
        { key: process.env.OPENROUTER_API_KEY } // ƒê·∫£m b·∫£o key t·ªìn t·∫°i
      ],
      models: [{ modelId: "arcee-ai/trinity-large-preview:free" }],
    },
  ],
});
```

### 2. "All API keys failed"

**Nguy√™n nh√¢n:** T·∫•t c·∫£ API keys ƒë·ªÅu fail (invalid, rate limit, etc.)

**Gi·∫£i ph√°p:**

1. Ki·ªÉm tra API keys c√≥ h·ª£p l·ªá kh√¥ng
2. Ki·ªÉm tra daily limits
3. Xem logs ƒë·ªÉ bi·∫øt l·ªói c·ª• th·ªÉ

```typescript
// Check key stats
const stats = aio.getKeyStats("openrouter");
console.log(stats);

// Reset daily counters n·∫øu c·∫ßn
aio.resetDailyCounters();
```

### 3. "All providers exhausted"

**Nguy√™n nh√¢n:** T·∫•t c·∫£ providers ƒë·ªÅu fail trong auto mode.

**Gi·∫£i ph√°p:**

1. Ki·ªÉm tra network connection
2. Ki·ªÉm tra API keys c·ªßa t·∫•t c·∫£ providers
3. Th·ª≠ direct mode ƒë·ªÉ debug

```typescript
// Th·ª≠ t·ª´ng provider ri√™ng l·∫ª
const response = await aio.chatCompletion({
  provider: "openrouter",
  model: "arcee-ai/trinity-large-preview:free",
  messages: [{ role: "user", content: "Test" }],
});
```

### 4. "Invalid request"

**Nguy√™n nh√¢n:** Request kh√¥ng h·ª£p l·ªá (validation failed).

**Gi·∫£i ph√°p:**

1. Ki·ªÉm tra messages kh√¥ng empty
2. Ki·ªÉm tra temperature trong range 0-2
3. Ki·ªÉm tra max_tokens > 0

```typescript
// Valid request
const response = await aio.chatCompletion({
  provider: "openrouter",
  model: "arcee-ai/trinity-large-preview:free",
  messages: [
    { role: "user", content: "Hello" } // Kh√¥ng ƒë∆∞·ª£c empty
  ],
  temperature: 0.7, // 0.0-2.0
  max_tokens: 100,  // > 0
});
```

### 5. Rate Limit Errors

**Nguy√™n nh√¢n:** V∆∞·ª£t qu√° rate limit c·ªßa provider.

**Gi·∫£i ph√°p:**

1. S·ª≠ d·ª•ng multiple API keys v·ªõi priority
2. Implement daily limits
3. S·ª≠ d·ª•ng auto mode ƒë·ªÉ fallback

```typescript
const aio = new AIO({
  providers: [
    {
      provider: "openrouter",
      apiKeys: [
        { key: "key1", priority: 100, dailyLimit: 100 },
        { key: "key2", priority: 50, dailyLimit: 100 },
        { key: "key3", priority: 10, dailyLimit: 100 },
      ],
      models: [{ modelId: "arcee-ai/trinity-large-preview:free" }],
    },
  ],
});
```

### 6. Streaming Errors

**Nguy√™n nh√¢n:** Stream b·ªã disconnect ho·∫∑c error.

**Gi·∫£i ph√°p:**

1. Handle stream errors properly
2. Implement retry logic
3. Use abort signal ƒë·ªÉ cleanup

```typescript
const stream = await aio.chatCompletionStream({
  provider: "groq",
  model: "llama-3.3-70b-versatile",
  messages: [{ role: "user", content: "Hello" }],
});

stream.on("error", (error) => {
  console.error("Stream error:", error);
  // Retry ho·∫∑c fallback
});

stream.on("end", () => {
  console.log("Stream completed");
});
```

### 7. Multimodal Errors (Google AI)

**Nguy√™n nh√¢n:** Format kh√¥ng ƒë√∫ng ho·∫∑c provider kh√¥ng h·ªó tr·ª£.

**Gi·∫£i ph√°p:**

1. Ch·ªâ s·ª≠ d·ª•ng v·ªõi Google AI
2. Ki·ªÉm tra MIME type h·ª£p l·ªá
3. Ki·ªÉm tra base64 encoding ƒë√∫ng

```typescript
// Ch·ªâ Google AI h·ªó tr·ª£ multimodal
const response = await aio.chatCompletion({
  provider: "google-ai", // Ph·∫£i l√† google-ai
  model: "gemini-1.5-flash",
  messages: [
    {
      role: "user",
      content: [
        { type: "text", text: "Describe image" },
        {
          type: "image",
          source: {
            type: "base64",
            media_type: "image/jpeg", // Valid MIME type
            data: base64String,       // Valid base64
          },
        },
      ],
    },
  ],
});
```

### 8. JSON Schema Errors

**Nguy√™n nh√¢n:** Schema kh√¥ng h·ª£p l·ªá ho·∫∑c AI kh√¥ng tu√¢n th·ªß.

**Gi·∫£i ph√°p:**

1. S·ª≠ d·ª•ng `strict: true` ƒë·ªÉ b·∫Øt bu·ªôc tu√¢n th·ªß
2. Ki·ªÉm tra schema h·ª£p l·ªá (JSON Schema format)
3. Th·ª≠ v·ªõi model t·ªët h∆°n

```typescript
const response = await aio.chatCompletion({
  provider: "openrouter",
  model: "arcee-ai/trinity-large-preview:free",
  messages: [
    {
      role: "user",
      content: "Extract: Name: John, Age: 30",
    },
  ],
  response_format: {
    type: "json_schema",
    json_schema: {
      name: "person",
      strict: true, // B·∫Øt bu·ªôc tu√¢n th·ªß
      schema: {
        type: "object",
        properties: {
          name: { type: "string" },
          age: { type: "number" },
        },
        required: ["name", "age"],
        additionalProperties: false,
      },
    },
  },
});
```

### 9. Memory/Token Limit Errors

**Nguy√™n nh√¢n:** Conversation qu√° d√†i, v∆∞·ª£t token limit.

**Gi·∫£i ph√°p:**

1. Gi·ªõi h·∫°n s·ªë messages
2. Summarize old messages
3. S·ª≠ d·ª•ng sliding window

```typescript
// Gi·ªõi h·∫°n 10 messages g·∫ßn nh·∫•t
const MAX_MESSAGES = 10;
const recentMessages = messages.slice(-MAX_MESSAGES);

const response = await aio.chatCompletion({
  provider: "openrouter",
  model: "arcee-ai/trinity-large-preview:free",
  messages: recentMessages,
});
```

### 10. Validation Errors

**Nguy√™n nh√¢n:** Config ho·∫∑c request kh√¥ng pass Zod validation.

**Gi·∫£i ph√°p:**

1. Ki·ªÉm tra error message ƒë·ªÉ bi·∫øt field n√†o sai
2. T·∫Øt validation n·∫øu c·∫ßn (kh√¥ng khuy·∫øn kh√≠ch)

```typescript
// T·∫Øt validation (kh√¥ng khuy·∫øn kh√≠ch)
const aio = new AIO({
  providers: [...],
  enableValidation: false,
});

// Ho·∫∑c fix validation error
const aio = new AIO({
  providers: [
    {
      provider: "openrouter",
      apiKeys: [
        { key: "valid-key" } // Kh√¥ng empty
      ],
      models: [
        { modelId: "valid-model" } // Kh√¥ng empty
      ],
    },
  ],
});
```

---

## üéØ Best Practices

### 1. S·ª≠ d·ª•ng Environment Variables

```typescript
// .env
OPENROUTER_API_KEY=sk-or-v1-xxxxx
GROQ_API_KEY=gsk_xxxxx
CEREBRAS_API_KEY=csk_xxxxx
GOOGLE_AI_API_KEY=AIzaSyxxxxx

// code
import dotenv from "dotenv";
dotenv.config();

const aio = new AIO({
  providers: [
    {
      provider: "openrouter",
      apiKeys: [{ key: process.env.OPENROUTER_API_KEY }],
      models: [{ modelId: "arcee-ai/trinity-large-preview:free" }],
    },
  ],
});
```

### 2. Implement Daily Limits

```typescript
// Reset counters m·ªói ng√†y (cron job)
import cron from "node-cron";

cron.schedule("0 0 * * *", () => {
  aio.resetDailyCounters();
  console.log("Daily counters reset");
});
```

### 3. Monitor Key Usage

```typescript
// Log key stats sau m·ªói request
const response = await aio.chatCompletion({...});

const stats = aio.getKeyStats("openrouter");
console.log(`Usage: ${stats.totalUsage}, Errors: ${stats.totalErrors}`);

// Alert khi usage cao
if (stats.totalUsage > 80) {
  console.warn("‚ö†Ô∏è High usage detected!");
}
```

### 4. Handle Errors Gracefully

```typescript
try {
  const response = await aio.chatCompletion({...});
} catch (error) {
  if (error instanceof AIOError) {
    const classification = AIOError.classify(error);
    
    if (classification.category === "rate_limit") {
      // ƒê·ª£i v√† retry
      await sleep(60000);
      return retry();
    } else if (classification.category === "auth") {
      // Alert admin
      notifyAdmin("Invalid API key");
    }
  }
  
  // Fallback response
  return { content: "Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra." };
}
```

### 5. Optimize Token Usage

```typescript
// Gi·ªõi h·∫°n conversation length
const MAX_MESSAGES = 20;
const messages = conversationHistory.slice(-MAX_MESSAGES);

// S·ª≠ d·ª•ng max_tokens ƒë·ªÉ control cost
const response = await aio.chatCompletion({
  provider: "openrouter",
  model: "arcee-ai/trinity-large-preview:free",
  messages,
  max_tokens: 500, // Gi·ªõi h·∫°n output
});
```

### 6. Use Auto Mode cho Production

```typescript
// Production config v·ªõi multiple providers
const aio = new AIO({
  providers: [
    {
      provider: "groq",
      apiKeys: [
        { key: process.env.GROQ_KEY_1, priority: 100 },
        { key: process.env.GROQ_KEY_2, priority: 50 },
      ],
      models: [{ modelId: "llama-3.3-70b-versatile" }],
      priority: 100,
    },
    {
      provider: "cerebras",
      apiKeys: [{ key: process.env.CEREBRAS_KEY }],
      models: [{ modelId: "llama3.1-8b" }],
      priority: 80,
    },
    {
      provider: "google-ai",
      apiKeys: [{ key: process.env.GOOGLE_AI_KEY }],
      models: [{ modelId: "gemini-1.5-flash" }],
      priority: 60,
    },
  ],
  autoMode: true,
  maxRetries: 5,
  retryDelay: 2000,
});
```

### 7. Implement Caching

```typescript
import NodeCache from "node-cache";

const cache = new NodeCache({ stdTTL: 3600 }); // 1 hour

async function getChatResponse(prompt: string) {
  // Check cache
  const cached = cache.get(prompt);
  if (cached) return cached;
  
  // Get from AI
  const response = await aio.chatCompletion({
    provider: "openrouter",
    model: "arcee-ai/trinity-large-preview:free",
    messages: [{ role: "user", content: prompt }],
  });
  
  // Cache result
  cache.set(prompt, response);
  return response;
}
```

### 8. Use Streaming cho Long Responses

```typescript
// Streaming t·ªët h∆°n cho long-form content
const stream = await aio.chatCompletionStream({
  provider: "groq",
  model: "llama-3.3-70b-versatile",
  messages: [
    { role: "user", content: "Vi·∫øt m·ªôt b√†i lu·∫≠n d√†i v·ªÅ AI" }
  ],
});

// Send to client real-time
for await (const chunk of stream) {
  res.write(chunk);
}
```

### 9. Validate User Input

```typescript
import { z } from "zod";

const UserInputSchema = z.object({
  message: z.string().min(1).max(1000),
  temperature: z.number().min(0).max(2).optional(),
});

// Validate before sending to AI
const input = UserInputSchema.parse(req.body);

const response = await aio.chatCompletion({
  provider: "openrouter",
  model: "arcee-ai/trinity-large-preview:free",
  messages: [{ role: "user", content: input.message }],
  temperature: input.temperature,
});
```

### 10. Implement Rate Limiting

```typescript
import rateLimit from "express-rate-limit";

const limiter = rateLimit({
  windowMs: 15 * 60 * 1000, // 15 minutes
  max: 100, // 100 requests per window
});

app.use("/api/chat", limiter);

app.post("/api/chat", async (req, res) => {
  const response = await aio.chatCompletion({...});
  res.json(response);
});
```

---

## üìö T√†i li·ªáu tham kh·∫£o

### Provider Documentation

- **OpenRouter**: https://openrouter.ai/docs
- **Groq**: https://console.groq.com/docs
- **Cerebras**: https://inference-docs.cerebras.ai/
- **Google AI**: https://ai.google.dev/docs

### Free Models

#### OpenRouter (30+ free models)

- `arcee-ai/trinity-large-preview:free`
- `openrouter/pony-alpha`
- `meta-llama/llama-3.2-3b-instruct:free`
- `google/gemini-flash-1.5:free`
- Xem th√™m: https://openrouter.ai/models?order=newest&supported_parameters=tools&max_price=0

#### Groq

- `llama-3.3-70b-versatile`
- `llama-3.1-8b-instant`
- `mixtral-8x7b-32768`

#### Cerebras

- `llama3.1-8b`
- `llama3.1-70b`

#### Google AI

- `gemini-1.5-flash` (Free tier: 15 RPM, 1M TPM)
- `gemini-1.5-pro` (Free tier: 2 RPM, 32K TPM)

---

## üìû Support

- **GitHub Issues**: https://github.com/yourusername/aio-llm/issues
- **Documentation**: https://github.com/yourusername/aio-llm#readme
- **Examples**: https://github.com/yourusername/aio-llm/tree/main/examples

---

## üìÑ License

MIT License - Xem file LICENSE ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt.

---

**Ch√∫c b·∫°n code vui v·∫ª! üöÄ**
